{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project - Facial Expressions Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5E2JqCl2Hv6PxHvO4q0EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavishabhambri/Project---Facial-Expressions-Recognition/blob/master/Project_Facial_Expressions_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC7ecFU8eU7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "51dc9ce6-eae9-4269-cc3e-037eb345a60a"
      },
      "source": [
        "!git clone https://github.com/muxspace/facial_expressions.git  #this clones the data for us."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'facial_expressions'...\n",
            "remote: Enumerating objects: 14214, done.\u001b[K\n",
            "remote: Total 14214 (delta 0), reused 0 (delta 0), pack-reused 14214\u001b[K\n",
            "Receiving objects: 100% (14214/14214), 239.65 MiB | 13.82 MiB/s, done.\n",
            "Resolving deltas: 100% (223/223), done.\n",
            "Checking out files: 100% (13996/13996), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NYL0JhOefrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "data = {}  #this contains the keys - expression and the values are the images corresponding to the expression.\n",
        "with open('/content/facial_expressions/data/legend.csv') as f:  #opening the legend.csv file\n",
        "  reader = csv.reader(f)  #this reads the file from top, but we dont want the headers in our data, so move this reader object to the next\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    key = row[2].lower()  #keys - expression which are = row[0] in lower case so that 'anger' and 'ANGER' are considered same.\n",
        "    if key in data:\n",
        "      data[key].append(row[1])  #if it already exists in the dictionary then just append the image name as valuse\n",
        "    else:\n",
        "      data[key] = [row[1]]  #else make a new list with the image name."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CON2ciKBgELE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "04510e48-f934-4169-83b8-ef25eb89a784"
      },
      "source": [
        "emotions_list = list(data.keys())  #this gives expressions only in lower format as we did above in lowercase.\n",
        "emotions_list"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger',\n",
              " 'surprise',\n",
              " 'disgust',\n",
              " 'fear',\n",
              " 'neutral',\n",
              " 'happiness',\n",
              " 'sadness',\n",
              " 'contempt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnNxcQGAioN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding sub directories for traning and testing data inside master_data directory\n",
        "import os\n",
        "\n",
        "os.mkdir('master_data')\n",
        "os.mkdir('master_data/training')\n",
        "os.mkdir('master_data/testing')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH5YR-tmmGR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inside the training and testing folders create the folders for expressions\n",
        "for emotion in emotions_list:\n",
        "  os.mkdir(os.path.join('master_data/training/', emotion))\n",
        "  os.mkdir(os.path.join('master_data/testing/', emotion))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fSQTnI0mfp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding images to these emotions in training and testing folders\n",
        "from shutil import copyfile\n",
        "split_size = 0.8\n",
        "\n",
        "for emotion, images in data.items():\n",
        "  train_size = int(split_size * len(images))\n",
        "  train_images = images[:train_size]\n",
        "  test_images = images[train_size: ]\n",
        "\n",
        "  for image in train_images:\n",
        "    source = os.path.join('/content/facial_expressions/images', image)\n",
        "    dest = os.path.join('/content/master_data/training', emotion, image)\n",
        "    copyfile(source, dest)\n",
        "\n",
        "  for image in test_images:\n",
        "    source = os.path.join('/content/facial_expressions/images', image)\n",
        "    dest = os.path.join('/content/master_data/testing', emotion, image)\n",
        "    copyfile(source, dest)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dac863YXnlzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing all the dependencies\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X9_pNBFtoRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting the model\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    Conv2D(16, (3, 3), activation = 'relu', input_shape = (100, 100, 3)),\n",
        "                                    MaxPooling2D(2, 2),\n",
        "                                    Conv2D(32, (3, 3), activation = 'relu'),\n",
        "                                    MaxPooling2D(2, 2),\n",
        "                                    Conv2D(64, (3, 3), activation = 'relu'),\n",
        "                                    MaxPooling2D(2, 2),\n",
        "                                    Flatten(),\n",
        "                                    Dense(512, activation = 'relu'),\n",
        "                                    Dense(8, activation = 'softmax')\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T953EbCiuo_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "bae4a2df-30b4-4eaf-d7a5-b4da69bdf9f8"
      },
      "source": [
        "model.compile(optimizer= Adam(lr = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 98, 98, 16)        448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 49, 49, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 47, 47, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 21, 21, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3277312   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 3,305,000\n",
            "Trainable params: 3,305,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biaTBPU1vASv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = '/content/master_data/training'\n",
        "test_dir = '/content/master_data/testing'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQKCAZmvC316",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "659d3b3c-668c-406a-94e2-2de77624af72"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale= 1.0/255)  #normalising the data to same pixels\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size = (100, 100),\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    batch_size = 128)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale= 1.0/255)  #normalising the data to same pixels\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                    target_size = (100, 100),\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    batch_size = 128)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10941 images belonging to 8 classes.\n",
            "Found 2742 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp6VrXMXC4D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Early Stopping callback - to stop early when the validation accuracy does not improve much further\n",
        "es = EarlyStopping(monitor = 'val_acc', patience = 2, min_delta = 0.01)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDHBt_xwC4Bn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "20b1629b-503c-4455-ed2d-73986c5fda5d"
      },
      "source": [
        "model.fit_generator(train_generator,\n",
        "                    epochs = 10,\n",
        "                    verbose = 1,\n",
        "                    validation_data = test_generator,\n",
        "                    callbacks = [es])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-a67ff5996604>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.2914 - accuracy: 0.5288WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 19s 222ms/step - loss: 1.2914 - accuracy: 0.5288 - val_loss: 1.0286 - val_accuracy: 0.5635\n",
            "Epoch 2/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.6561WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 19s 216ms/step - loss: 0.8996 - accuracy: 0.6561 - val_loss: 0.9686 - val_accuracy: 0.6550\n",
            "Epoch 3/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.7756 - accuracy: 0.7311WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 213ms/step - loss: 0.7756 - accuracy: 0.7311 - val_loss: 0.8663 - val_accuracy: 0.6648\n",
            "Epoch 4/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.7130 - accuracy: 0.7543WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 214ms/step - loss: 0.7130 - accuracy: 0.7543 - val_loss: 1.1249 - val_accuracy: 0.5912\n",
            "Epoch 5/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.7596WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 211ms/step - loss: 0.6915 - accuracy: 0.7596 - val_loss: 1.0802 - val_accuracy: 0.6229\n",
            "Epoch 6/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7728WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 213ms/step - loss: 0.6627 - accuracy: 0.7728 - val_loss: 1.2074 - val_accuracy: 0.6255\n",
            "Epoch 7/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.7812WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 212ms/step - loss: 0.6344 - accuracy: 0.7812 - val_loss: 1.2262 - val_accuracy: 0.6313\n",
            "Epoch 8/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.7810WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 210ms/step - loss: 0.6274 - accuracy: 0.7810 - val_loss: 1.1770 - val_accuracy: 0.6346\n",
            "Epoch 9/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.7893WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 209ms/step - loss: 0.6048 - accuracy: 0.7893 - val_loss: 1.3182 - val_accuracy: 0.6360\n",
            "Epoch 10/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.7970WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "86/86 [==============================] - 18s 209ms/step - loss: 0.5800 - accuracy: 0.7970 - val_loss: 1.3206 - val_accuracy: 0.6349\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b203bb128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAgamEZ1C35g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}